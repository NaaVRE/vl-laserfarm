{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7404fb96",
   "metadata": {},
   "source": [
    "# Laserfarm: LiDAR point cloud analysis for macro-ecology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8c217415279f91",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4afda19c5108b60",
   "metadata": {},
   "source": [
    "### User parameters\n",
    "\n",
    "Defines the parameters that can be set by users when executing the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a04045277b08e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (DO NOT containerize this cell)\n",
    "\n",
    "# \n",
    "param_laz_urls = [\"https://basisdata.nl/hwh-ahn/AHN6/01_LAZ/AHN6_2025_C_168000_520000.LAZ\", \"https://basisdata.nl/hwh-ahn/AHN6/01_LAZ/AHN6_2025_C_168000_519000.LAZ\"]\n",
    "\n",
    "# Data handling parameters\n",
    "param_minio_endpoint = 'scruffy.lab.uvalight.net:9000'\n",
    "param_minio_public_bucket = 'naa-vre-public'\n",
    "param_minio_virtual_lab_bucket = 'naa-vre-laserfarm'\n",
    "\n",
    "# Laserfarm parameters\n",
    "param_feature_name = 'perc_95_normalized_height'\n",
    "param_validate_precision = '0.001'\n",
    "param_tile_mesh_size = '10.'\n",
    "param_filter_type = 'select_equal'\n",
    "param_attribute = 'raw_classification'\n",
    "param_min_x = '-113107.81'  # EPSG:28992\n",
    "param_max_x = '398892.19'  # EPSG:28992\n",
    "param_min_y = '214783.87'  # EPSG:28992\n",
    "param_max_y = '726783.87'  # EPSG:28992\n",
    "param_n_tiles_side = '512'\n",
    "param_apply_filter_value = '1'\n",
    "param_processed_files_record_file = 'processed_files_log.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff0d935-0d3c-49ca-ae44-6af32bd5d003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Secrets (DO NOT containerize this cell)\n",
    "from SecretsProvider import SecretsProvider\n",
    "from getpass import getpass\n",
    "\n",
    "secrets_provider = SecretsProvider(input_func=getpass)\n",
    "secret_minio_access_key = secrets_provider.get_secret('secret_minio_access_key')\n",
    "secret_minio_secret_key = secrets_provider.get_secret('secret_minio_secret_key')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68092526ef49c9c",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "\n",
    "The following cells install extra dependencies that are not included in the Laserfarm flavor by default, and import the libraries used in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18041536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (DO NOT containerize this cell)\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "from laserfarm import DataProcessing, GeotiffWriter, Retiler\n",
    "from laserfarm.remote_utils import get_wdclient, list_remote\n",
    "from minio import Minio\n",
    "from minio.error import S3Error\n",
    "import laspy\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd0baf8fbe0c410",
   "metadata": {},
   "source": [
    "### Global configuration\n",
    "\n",
    "The following variable are used throughout the code. They are intended to be edited by developers who the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbff2bb-39bf-4cf9-880d-15e80734b6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (DO NOT containerize this cell)\n",
    "\n",
    "conf_local_tmp = '/tmp/data'\n",
    "conf_local_path_raw = os.path.join(conf_local_tmp, 'raw')\n",
    "conf_local_path_split = os.path.join(conf_local_tmp, 'split')\n",
    "conf_local_path_retiled = os.path.join(conf_local_tmp, 'retiled')\n",
    "conf_local_path_targets = os.path.join(conf_local_tmp, 'targets')\n",
    "conf_local_path_geotiff = os.path.join(conf_local_tmp, 'geotiff')\n",
    "conf_local_path_figures = os.path.join(conf_local_tmp, 'figures')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7ea51c2f2e5908",
   "metadata": {},
   "source": [
    "## Workflow steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f50d4b-411a-4033-a780-674259df1f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processed Files Tracker\n",
    "def get_minio_file_as_set(bucket_name: str, object_name: str) -> set[str]:\n",
    "    response = None\n",
    "    try:\n",
    "        response = minio_client.get_object(bucket_name, object_name)\n",
    "        content = response.data.decode(\"utf-8\")\n",
    "        return {line.strip() for line in content.splitlines() if line.strip()}\n",
    "    \n",
    "    except S3Error as e:\n",
    "        if e.code == \"NoSuchKey\":\n",
    "            return set()\n",
    "        raise e\n",
    "        \n",
    "    finally:\n",
    "        if response:\n",
    "            response.close()\n",
    "            response.release_conn()\n",
    "\n",
    "minio_client = Minio(\n",
    "    param_minio_endpoint, \n",
    "    access_key=secret_minio_access_key,\n",
    "    secret_key=secret_minio_secret_key,\n",
    "    secure=True\n",
    ")\n",
    "\n",
    "processed_files  = get_minio_file_as_set(param_minio_virtual_lab_bucket, f\"{param_feature_name}/{param_processed_files_record_file}\")\n",
    "files_to_process = list(set(param_laz_urls) - processed_files)\n",
    "print(f\"Found {len(param_laz_urls)} files to process, of which {len(processed_files)} were already processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03d1b76",
   "metadata": {},
   "source": [
    "### Fetch laz files from remote storage\n",
    "\n",
    "This cell downloads `.laz` files from the remote MinIO storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573679af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S1 Fetch laz files\n",
    "import urllib.request\n",
    "\n",
    "os.makedirs(conf_local_path_raw, exist_ok=True)\n",
    "\n",
    "laz_urls = files_to_process\n",
    "\n",
    "raw_laz_files = []\n",
    "for laz_url in laz_urls:\n",
    "    print(f\"retrieving file from {laz_url}\")\n",
    "    filename = laz_url.rpartition('/')[-1] \n",
    "    file_location = f\"{conf_local_path_raw}/{filename}\"\n",
    "    # urllib.request.urlretrieve(laz_url, f\"{filename}\")\n",
    "    urllib.request.urlretrieve(laz_url, file_location)\n",
    "\n",
    "    raw_laz_files.append(file_location)\n",
    "\n",
    "print(raw_laz_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8890db-2f3f-4aaf-a2a7-730df1cdc8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 Retile laz files\n",
    "# base image: laserfarm\n",
    "\n",
    "grid_retile = {\n",
    "    'min_x': float(param_min_x),\n",
    "    'max_x': float(param_max_x),\n",
    "    'min_y': float(param_min_y),\n",
    "    'max_y': float(param_max_y),\n",
    "    'n_tiles_side': int(param_n_tiles_side),\n",
    "    }\n",
    "\n",
    "retiling_input = {\n",
    "    'setup_local_fs': {\n",
    "        'input_folder': conf_local_path_split,\n",
    "        'output_folder': conf_local_path_retiled,\n",
    "        },\n",
    "    'set_grid': grid_retile,\n",
    "    'split_and_redistribute': {},\n",
    "    'validate': {},\n",
    "    }\n",
    "\n",
    "os.makedirs(conf_local_path_retiled, exist_ok=True)\n",
    "tiles = []\n",
    "\n",
    "for file in raw_laz_files:\n",
    "    base_name = os.path.splitext(os.path.basename(file))[0]\n",
    "    retile_record_filename = os.path.join(\n",
    "        conf_local_path_retiled,\n",
    "        f'{base_name}_retile_record.js',\n",
    "        )\n",
    "    if not os.path.isfile(retile_record_filename):\n",
    "        print(f'Retiling {file}')\n",
    "        retiler = Retiler(file, label=file).config(retiling_input)\n",
    "        retiler.run()\n",
    "    else:\n",
    "        print(\n",
    "            f'Skipping retiling of {file} because {retile_record_filename} already exists'\n",
    "            )\n",
    "    # load filenames from retile record\n",
    "    with open(retile_record_filename, 'r') as f:\n",
    "        retile_record = json.load(f)\n",
    "    \n",
    "    tiles += retile_record['redistributed_to']\n",
    "    \n",
    "\n",
    "print(retile_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45891c72fb134376",
   "metadata": {},
   "source": [
    "### Extract features from tiles\n",
    "\n",
    "Run the feature extraction for each tile. The features are extracted using [laserchicken](https://github.com/eEcoLiDAR/laserchicken)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab006ce8-f290-414d-a1be-1dcd3b2f101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S5 Extract features\n",
    "# base image: laserfarm\n",
    "\n",
    "feature_files = []\n",
    "\n",
    "for i, tile in enumerate(tiles):\n",
    "    grid_feature = {\n",
    "        'min_x': float(param_min_x),\n",
    "        'max_x': float(param_max_x),\n",
    "        'min_y': float(param_min_y),\n",
    "        'max_y': float(param_max_y),\n",
    "        'n_tiles_side': int(param_n_tiles_side),\n",
    "        }\n",
    "\n",
    "    feature_extraction_input = {\n",
    "        'setup_local_fs': {\n",
    "            'input_folder': conf_local_path_retiled,\n",
    "            'output_folder': conf_local_path_targets,\n",
    "            },\n",
    "        'load': {'attributes': [param_attribute]},\n",
    "        'normalize': 1,\n",
    "        'apply_filter': {\n",
    "            'filter_type': param_filter_type,\n",
    "            'attribute': param_attribute,\n",
    "            'value': [int(param_apply_filter_value)],\n",
    "            #ground surface (2), water (9), buildings (6), artificial objects (26), vegetation (?), and unclassified (1)\n",
    "            },\n",
    "        'generate_targets': {\n",
    "            'tile_mesh_size': float(param_tile_mesh_size),\n",
    "            'validate': True,\n",
    "            'validate_precision': float(param_validate_precision),\n",
    "            **grid_feature\n",
    "            },\n",
    "        'extract_features': {\n",
    "            'feature_names': [param_feature_name],\n",
    "            'volume_type': 'cell',\n",
    "            'volume_size': float(param_tile_mesh_size),\n",
    "            },\n",
    "        'export_targets': {\n",
    "            'attributes': [param_feature_name],\n",
    "            'multi_band_files': False,\n",
    "            },\n",
    "        }\n",
    "    idx = (tile.split('_')[1:])\n",
    "\n",
    "    target_file = os.path.join(\n",
    "        conf_local_path_targets, param_feature_name, tile + '.ply'\n",
    "        )\n",
    "    print(target_file)\n",
    "\n",
    "    if not os.path.isfile(target_file):\n",
    "        processing = DataProcessing(tile, tile_index=idx, label=tile).config(\n",
    "            feature_extraction_input\n",
    "            )\n",
    "        processing.run()\n",
    "    else:\n",
    "        print(\n",
    "            f'Skipping features extraction for {tile} ({i + 1} of {len(tiles)}) because {target_file} already exists'\n",
    "            )\n",
    "\n",
    "    feature_files.append(target_file)\n",
    "\n",
    "print(feature_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667f6cb2-94b5-4a6d-9003-d99de31493e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save to MinIO\n",
    "def get_minio_file_as_set() -> set[str]:\n",
    "    response = None\n",
    "    try:\n",
    "        response = minio_client.get_object(param_minio_virtual_lab_bucket, param_processed_files_record_file)\n",
    "        content = response.data.decode(\"utf-8\")\n",
    "        return {line.strip() for line in content.splitlines() if line.strip()}\n",
    "    \n",
    "    except S3Error as e:\n",
    "        if e.code == \"NoSuchKey\":\n",
    "            return set()\n",
    "        raise e\n",
    "        \n",
    "    finally:\n",
    "        if response:\n",
    "            response.close()\n",
    "            response.release_conn()\n",
    "\n",
    "def add_to_processed_files_log(newly_processed_files : set[str]):\n",
    "    files_processed_earlier = get_minio_file_as_set()\n",
    "    processed_files = files_processed_earlier |  newly_processed_files\n",
    "    set_as_string = \"\\n\".join(processed_files)\n",
    "    set_as_bytes = set_as_string.encode(\"utf-8\") \n",
    "    minio_client.put_object(\n",
    "        bucket_name=param_minio_virtual_lab_bucket, \n",
    "        object_name=f\"{param_feature_name}/{param_processed_files_record_file}\", \n",
    "        data=io.BytesIO(set_as_bytes), length=len(set_as_bytes))\n",
    "\n",
    "def copy_to_minio(filepath : str):\n",
    "    filename = filepath.replace(conf_local_path_targets,'')\n",
    "    minio_client.fput_object(bucket_name=param_minio_virtual_lab_bucket, file_path=filepath, object_name=filename)\n",
    "    return\n",
    "\n",
    "minio_client = Minio(\n",
    "    param_minio_endpoint, \n",
    "    access_key=secret_minio_access_key,\n",
    "    secret_key=secret_minio_secret_key,\n",
    "    secure=True\n",
    ")\n",
    "\n",
    "processed_laz_filenames = set(laz_urls)\n",
    "add_to_processed_files_log(processed_laz_filenames)\n",
    "\n",
    "for filepath in feature_files:\n",
    "    copy_to_minio(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf0e09b-b117-43dd-8582-bd8c38e00bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:laserfarm]",
   "language": "python",
   "name": "conda-env-laserfarm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
