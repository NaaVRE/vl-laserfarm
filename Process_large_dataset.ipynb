{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7404fb96",
   "metadata": {},
   "source": [
    "# Laserfarm: LiDAR point cloud analysis for macro-ecology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8c217415279f91",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4afda19c5108b60",
   "metadata": {},
   "source": [
    "### User parameters\n",
    "\n",
    "Defines the parameters that can be set by users when executing the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a04045277b08e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (DO NOT containerize this cell)\n",
    "\n",
    "# Data handling parameters\n",
    "param_minio_endpoint = 'scruffy.lab.uvalight.net:9000'\n",
    "param_minio_public_bucket = 'naa-vre-public'\n",
    "param_minio_virtual_lab_bucket = 'naa-vre-laserfarm'\n",
    "\n",
    "# Laserfarm parameters\n",
    "param_feature_name = 'perc_95_normalized_height'\n",
    "param_validate_precision = '0.001'\n",
    "param_tile_mesh_size = '10.'\n",
    "param_filter_type = 'select_equal'\n",
    "param_attribute = 'raw_classification'\n",
    "param_min_x = '-113107.81'  # EPSG:28992\n",
    "param_max_x = '398892.19'  # EPSG:28992\n",
    "param_min_y = '214783.87'  # EPSG:28992\n",
    "param_max_y = '726783.87'  # EPSG:28992\n",
    "param_n_tiles_side = '512'\n",
    "param_apply_filter_value = '1'\n",
    "param_processed_files_record_file = 'processed_files_log.txt'\n",
    "param_files_to_process_file = 'AHN6_url_test.txt'\n",
    "param_max_tiles = 2\n",
    "param_max_nodes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff0d935-0d3c-49ca-ae44-6af32bd5d003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Secrets (DO NOT containerize this cell)\n",
    "from SecretsProvider import SecretsProvider\n",
    "from getpass import getpass\n",
    "\n",
    "secrets_provider = SecretsProvider(input_func=getpass)\n",
    "secret_minio_access_key = secrets_provider.get_secret('secret_minio_access_key')\n",
    "secret_minio_secret_key = secrets_provider.get_secret('secret_minio_secret_key')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68092526ef49c9c",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "\n",
    "The following cells install extra dependencies that are not included in the Laserfarm flavor by default, and import the libraries used in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18041536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (DO NOT containerize this cell)\n",
    "\n",
    "import json\n",
    "import os\n",
    "import laspy\n",
    "import io\n",
    "import random\n",
    "import math\n",
    "\n",
    "from laserfarm import DataProcessing, GeotiffWriter, Retiler\n",
    "from laserfarm.remote_utils import get_wdclient, list_remote\n",
    "from minio import Minio\n",
    "from minio.error import S3Error\n",
    "from typing import List\n",
    "from urllib import request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd0baf8fbe0c410",
   "metadata": {},
   "source": [
    "### Global configuration\n",
    "\n",
    "The following variable are used throughout the code. They are intended to be edited by developers who the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbff2bb-39bf-4cf9-880d-15e80734b6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (DO NOT containerize this cell)\n",
    "\n",
    "conf_local_tmp = '/tmp/data'\n",
    "conf_local_path_raw = os.path.join(conf_local_tmp, 'raw')\n",
    "conf_local_path_retiled = os.path.join(conf_local_tmp, 'retiled')\n",
    "conf_local_path_targets = os.path.join(conf_local_tmp, 'targets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7ea51c2f2e5908",
   "metadata": {},
   "source": [
    "## Workflow steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f50d4b-411a-4033-a780-674259df1f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datafiles processing manager\n",
    "def get_filename_batches_to_process() -> List[List[str]]:\n",
    "    filenames_to_process = get_minio_file_as_set(f\"{param_feature_name}/{param_files_to_process_file}\")\n",
    "    unprocessed_filenames = keep_unprocessed_filenames(filenames_to_process)\n",
    "    if not unprocessed_filenames:\n",
    "        raise ValueError(f\"The list of unprocessed_filenames is empty. Cannot proceed with processing.\")\n",
    "    random.shuffle(unprocessed_filenames) \n",
    "    selected_filenames = unprocessed_filenames[:param_max_tiles]\n",
    "    filename_batches = split_into_batches(selected_filenames)\n",
    "    print(f\"Files to process: {len(filenames_to_process)}, of which {len(unprocessed_filenames)} are unprocessed. Preparing {len(selected_filenames)} files in {len(filename_batches)} batches.\")\n",
    "    return filename_batches\n",
    "\n",
    "def get_minio_file_as_set(filename: str) -> set[str]:\n",
    "    response = None\n",
    "    try:\n",
    "        response = minio_client.get_object(param_minio_virtual_lab_bucket, filename)\n",
    "        content = response.data.decode(\"utf-8\")\n",
    "        return {line.strip() for line in content.splitlines() if line.strip()}\n",
    "    \n",
    "    except S3Error as e:\n",
    "        if e.code == \"NoSuchKey\":\n",
    "            return set()\n",
    "        raise e\n",
    "        \n",
    "    finally:\n",
    "        if response:\n",
    "            response.close()\n",
    "            response.release_conn()\n",
    "\n",
    "minio_client = Minio(\n",
    "    param_minio_endpoint, \n",
    "    access_key=secret_minio_access_key,\n",
    "    secret_key=secret_minio_secret_key,\n",
    "    secure=True\n",
    ")\n",
    "    \n",
    "def keep_unprocessed_filenames(filenames : List[str]) -> List[str]:\n",
    "    processed_files : set[str] = get_minio_file_as_set(f\"{param_feature_name}/{param_processed_files_record_file}\")\n",
    "    files_to_process = list(set(filenames) - processed_files)\n",
    "    return files_to_process\n",
    "\n",
    "def split_into_batches(filenames : List[str]):\n",
    "    batch_size = math.ceil(len(filenames)/param_max_nodes)\n",
    "    return[filenames[i:i + batch_size] for i in range(0, len(filenames), batch_size)]\n",
    "\n",
    "def prepare_directories():\n",
    "    os.makedirs(conf_local_path_raw, exist_ok=True)\n",
    "    os.makedirs(conf_local_path_retiled, exist_ok=True)\n",
    "\n",
    "las_data_filename_batches: List[List[str]] = get_filename_batches_to_process()\n",
    "prepare_directories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21b702e-bb64-46e4-936a-792a764c3af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laserfarm runner\n",
    "def process(batch : List[str]):\n",
    "    for url in batch:\n",
    "        process_data(url)\n",
    "\n",
    "def process_data(url : str):\n",
    "    downloaded_filepath = fetch_file(url, output_folder=conf_local_path_raw)\n",
    "    retiled_filepaths = retile_laz_file(raw_laz_filepath=downloaded_filepath, input_folder=conf_local_path_raw, output_folder=conf_local_path_retiled)\n",
    "    feature_filepaths = extract_features(retiled_filepaths, input_folder=conf_local_path_retiled, output_folder=conf_local_path_targets)\n",
    "    for filepath in feature_filepaths:\n",
    "        copy_to_minio(filepath, input_folder=conf_local_path_targets)\n",
    "    add_to_processed_log(url)\n",
    "\n",
    "def fetch_file(url : str, output_folder : str) -> str:\n",
    "    filename = url.rpartition('/')[-1] \n",
    "    filepath = f\"{output_folder}/{filename}\"\n",
    "    request.urlretrieve(url, filepath)\n",
    "    return filepath\n",
    "\n",
    "def retile_laz_file(raw_laz_filepath : str, input_folder : str, output_folder : str) -> List[str]:\n",
    "    grid_retile = {\n",
    "        'min_x': float(param_min_x),\n",
    "        'max_x': float(param_max_x),\n",
    "        'min_y': float(param_min_y),\n",
    "        'max_y': float(param_max_y),\n",
    "        'n_tiles_side': int(param_n_tiles_side),\n",
    "        }\n",
    "\n",
    "    retiling_input = {\n",
    "        'setup_local_fs': {\n",
    "            'input_folder': input_folder,\n",
    "            'output_folder': output_folder,\n",
    "            },\n",
    "        'set_grid': grid_retile,\n",
    "        'split_and_redistribute': {},\n",
    "        'validate': {},\n",
    "        }\n",
    "    \n",
    "    tiles = []\n",
    "    base_name = os.path.splitext(os.path.basename(raw_laz_filepath))[0]\n",
    "    retile_record_filename = os.path.join(\n",
    "        output_folder,\n",
    "        f'{base_name}_retile_record.js',\n",
    "        )\n",
    "    if not os.path.isfile(retile_record_filename):\n",
    "        print(f'Retiling {raw_laz_filepath}')\n",
    "        retiler = Retiler(raw_laz_filepath, label=raw_laz_filepath).config(retiling_input)\n",
    "        retiler.run()\n",
    "    else:\n",
    "        print(\n",
    "            f'Skipping retiling of {raw_laz_filepath} because {retile_record_filename} already exists'\n",
    "            )\n",
    "    # load filenames from retile record\n",
    "    with open(retile_record_filename, 'r') as f:\n",
    "        retile_record = json.load(f)\n",
    "    \n",
    "    tiles += retile_record['redistributed_to']\n",
    "    return tiles\n",
    "\n",
    "def extract_features(retiled_laz_filepaths : List[str], input_folder : str, output_folder : str):\n",
    "    feature_files = []\n",
    "    for i, tile in enumerate(retiled_laz_filepaths):\n",
    "        grid_feature = {\n",
    "            'min_x': float(param_min_x),\n",
    "            'max_x': float(param_max_x),\n",
    "            'min_y': float(param_min_y),\n",
    "            'max_y': float(param_max_y),\n",
    "            'n_tiles_side': int(param_n_tiles_side),\n",
    "            }\n",
    "    \n",
    "        feature_extraction_input = {\n",
    "            'setup_local_fs': {\n",
    "                'input_folder': input_folder,\n",
    "                'output_folder': output_folder,\n",
    "                },\n",
    "            'load': {'attributes': [param_attribute]},\n",
    "            'normalize': 1,\n",
    "            'apply_filter': {\n",
    "                'filter_type': param_filter_type,\n",
    "                'attribute': param_attribute,\n",
    "                'value': [int(param_apply_filter_value)],\n",
    "                #ground surface (2), water (9), buildings (6), artificial objects (26), vegetation (?), and unclassified (1)\n",
    "                },\n",
    "            'generate_targets': {\n",
    "                'tile_mesh_size': float(param_tile_mesh_size),\n",
    "                'validate': True,\n",
    "                'validate_precision': float(param_validate_precision),\n",
    "                **grid_feature\n",
    "                },\n",
    "            'extract_features': {\n",
    "                'feature_names': [param_feature_name],\n",
    "                'volume_type': 'cell',\n",
    "                'volume_size': float(param_tile_mesh_size),\n",
    "                },\n",
    "            'export_targets': {\n",
    "                'attributes': [param_feature_name],\n",
    "                'multi_band_files': False,\n",
    "                },\n",
    "            }\n",
    "        idx = (tile.split('_')[1:])\n",
    "    \n",
    "        target_file = os.path.join(\n",
    "            output_folder, param_feature_name, tile + '.ply'\n",
    "            )\n",
    "        print(target_file)\n",
    "    \n",
    "        if not os.path.isfile(target_file):\n",
    "            processing = DataProcessing(tile, tile_index=idx, label=tile).config(\n",
    "                feature_extraction_input\n",
    "                )\n",
    "            processing.run()\n",
    "        else:\n",
    "            print(\n",
    "                f'Skipping features extraction for {tile} because {target_file} already exists'\n",
    "                )\n",
    "    \n",
    "        feature_files.append(target_file)\n",
    "    \n",
    "    return feature_files\n",
    " \n",
    "def get_processed_file_log() -> set[str]:\n",
    "    response = None\n",
    "    try:\n",
    "        response = minio_client.get_object(bucket_name=param_minio_virtual_lab_bucket, object_name=f\"{param_feature_name}/{param_processed_files_record_file}\")\n",
    "        content = response.data.decode(\"utf-8\")\n",
    "        return {line.strip() for line in content.splitlines() if line.strip()}\n",
    "    \n",
    "    except S3Error as e:\n",
    "        if e.code == \"NoSuchKey\":\n",
    "            return set()\n",
    "        raise e\n",
    "        \n",
    "    finally:\n",
    "        if response:\n",
    "            response.close()\n",
    "            response.release_conn()\n",
    "\n",
    "def add_to_processed_log(url : str):\n",
    "    processed_files : set = get_processed_file_log()\n",
    "    processed_files.add(url)\n",
    "    set_as_string = \"\\n\".join(processed_files)\n",
    "    set_as_bytes = set_as_string.encode(\"utf-8\") \n",
    "    minio_client.put_object(\n",
    "        bucket_name=param_minio_virtual_lab_bucket, \n",
    "        object_name=f\"{param_feature_name}/{param_processed_files_record_file}\", \n",
    "        data=io.BytesIO(set_as_bytes), length=len(set_as_bytes))\n",
    "\n",
    "def copy_to_minio(filepath : str, input_folder : str):\n",
    "    filename = filepath.replace(input_folder,'')\n",
    "    minio_client.fput_object(bucket_name=param_minio_virtual_lab_bucket, file_path=filepath, object_name=filename)\n",
    "\n",
    "minio_client = Minio(\n",
    "    param_minio_endpoint, \n",
    "    access_key=secret_minio_access_key,\n",
    "    secret_key=secret_minio_secret_key,\n",
    "    secure=True\n",
    ")\n",
    "    \n",
    "for batch in las_data_filename_batches:\n",
    "    process(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf0e09b-b117-43dd-8582-bd8c38e00bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:laserfarm]",
   "language": "python",
   "name": "conda-env-laserfarm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
